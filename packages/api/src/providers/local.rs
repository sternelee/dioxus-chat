use super::base::*;
use crate::chat_service::{Message, ModelConfig, ProviderError, Tool, Usage};
use async_trait::async_trait;
use futures::{Stream, StreamExt};
use std::pin::Pin;
use tracing::{debug, warn};

pub struct LocalProvider {
    model_config: ModelConfig,
}

impl LocalProvider {
    pub fn new(model_config: ModelConfig) -> Self {
        Self { model_config }
    }

    pub fn from_config(model_config: ModelConfig) -> Result<Self, ProviderError> {
        Ok(Self::new(model_config))
    }

    fn generate_mock_response(&self, messages: &[Message]) -> String {
        // Simple mock response generation based on the last user message
        if let Some(last_message) = messages.last() {
            let content = last_message.as_concat_text();

            if content.to_lowercase().contains("hello") {
                return "Hello! I'm a local AI assistant. How can I help you today?".to_string();
            } else if content.to_lowercase().contains("weather") {
                return "I'm sorry, I don't have access to weather data. You might want to check a weather service for that information.".to_string();
            } else if content.to_lowercase().contains("code") {
                return "I can help you with coding questions! Please provide more details about what you'd like to know or what you're working on.".to_string();
            } else {
                format!("I received your message: '{}'. This is a mock response from the local model '{}'. In a real implementation, this would be generated by an actual language model running locally.",
                    content, self.model_config.model)
            }
        } else {
            "I'm ready to help! Please send me a message.".to_string()
        }
    }
}

#[async_trait]
impl Provider for LocalProvider {
    fn metadata() -> ProviderMetadata
    where
        Self: Sized,
    {
        ProviderMetadata {
            id: "local".to_string(),
            name: "Local Mock Provider".to_string(),
            description: "A mock local provider for development and testing purposes".to_string(),
            supports_streaming: true,
            supports_tools: false,
            supports_images: false,
            supports_audio: false,
            max_tokens: Some(4096),
            pricing: None,
        }
    }

    fn model_config(&self) -> &ModelConfig {
        &self.model_config
    }

    async fn complete(
        &self,
        request: CompletionRequest,
    ) -> Result<CompletionResponse, ProviderError> {
        debug!(
            "Local completion request for model: {}",
            self.model_config.model
        );

        warn!("Using mock local provider - responses are not generated by real AI models");

        let content = self.generate_mock_response(&request.messages);

        Ok(CompletionResponse {
            content,
            usage: Usage {
                prompt_tokens: 100,
                completion_tokens: 50,
                total_tokens: 150,
            },
            finish_reason: Some("stop".to_string()),
            tool_calls: None,
        })
    }

    async fn stream(
        &self,
        request: CompletionRequest,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<ChatChunk, ProviderError>> + Send>>, ProviderError>
    {
        debug!(
            "Local stream request for model: {}",
            self.model_config.model
        );

        warn!("Using mock local provider - streaming responses are simulated");

        let full_response = self.generate_mock_response(&request.messages);
        let chars: Vec<char> = full_response.chars().collect();

        let converted_stream = async_stream::stream! {
            let mut accumulated = String::new();

            for chunk_chars in chars.chunks(5) {
                let chunk_str: String = chunk_chars.iter().collect();
                accumulated.push_str(&chunk_str);

                yield Ok(ChatChunk {
                    content: Some(chunk_str.clone()),
                    delta: Some(chunk_str),
                    tool_calls: None,
                    finish_reason: None,
                    usage: None,
                });

                // Small delay to simulate streaming
                tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
            }

            yield Ok(ChatChunk {
                content: None,
                delta: None,
                tool_calls: None,
                finish_reason: Some("stop".to_string()),
                usage: Some(Usage {
                    prompt_tokens: 100,
                    completion_tokens: 50,
                    total_tokens: 150,
                }),
            });
        };

        Ok(Box::pin(converted_stream))
    }

    async fn validate_request(&self, request: &CompletionRequest) -> Result<(), ProviderError> {
        if request.messages.is_empty() {
            return Err(ProviderError {
                message: "No messages provided".to_string(),
                code: Some("empty_messages".to_string()),
            });
        }

        // Local provider doesn't support tools
        if request
            .tools
            .as_ref()
            .map_or(false, |tools| !tools.is_empty())
        {
            warn!("Local provider doesn't support tools - tool requests will be ignored");
        }

        Ok(())
    }
}

